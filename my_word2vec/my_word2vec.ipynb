{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import copy\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "# from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process_source_data():\n",
    "    # 该类的作用是读取原始数据，构建词典映射关系和单词词频\n",
    "    def __init__(self, data_path, vocab_size=10000):\n",
    "        with open(data_path, 'r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "        # 将单词切分\n",
    "        self.text = text.lower().split()\n",
    "        \n",
    "        # 选出最多的vocab_size个词\n",
    "        # 得到单词字典表，key是单词，value是次数\n",
    "        vocob_dict = dict(Counter(self.text).most_common(vocab_size-1))\n",
    "        # 把不常用的单词都编码为\"<UNK>\"\n",
    "        vocob_dict['<UNK>'] = len(text) - np.sum(list(vocob_dict.values()))       \n",
    "        \n",
    "        #构建映射关系\n",
    "        self.word2id = {word:i for i, word in enumerate(vocob_dict)}\n",
    "        self.id2word = {i:word for i, word in enumerate(vocob_dict)}     \n",
    "        \n",
    "        # 根据3/4率调整词频\n",
    "        word_counts = np.asarray(list(vocob_dict.values()))\n",
    "        self.word_freq = (word_counts / np.sum(word_counts))** (3./4.)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skip_gram_Dataset(Data.Dataset):\n",
    "    # 该类的作用是将文字转换为对应ID， 并返回给定idx时对应的训练数据\n",
    "    def __init__(self, text, word2id, word_freq, C=3, K=15, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.C = C  # 上下文窗口\n",
    "        self.K = K  # 负采样比例\n",
    "        \n",
    "        self.text_encoded = [word2id.get(word, word2id['<UNK>']) for word in text]\n",
    "        self.text_encoded = torch.tensor(self.text_encoded, dtype=torch.long)\n",
    "        \n",
    "        self.word_freq = torch.tensor(word_freq)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        对于给定的idx，返回对应的训练数据\n",
    "        - 中心词\n",
    "        - 这个单词附近的positive word\n",
    "        - 随机采样的K个单词作为negative word\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx]\n",
    "        pos_idx = list(range(idx - self.C, idx)) + list(range(idx + 1, idx + self.C + 1))\n",
    "        pos_idx = [i % len(self.text_encoded) for i in pos_idx]\n",
    "        pos_words = self.text_encoded[pos_idx]\n",
    "        \n",
    "        # torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None)\n",
    "        # 可以根据input的权重随机选出num_samples个input的下标\n",
    "        # replacement表示是否是有放回的抽取\n",
    "        select_weight = copy.deepcopy(self.word_freq)\n",
    "        select_weight[pos_words] = 0     # 去除背景词\n",
    "        select_weight[center_word] = 0   # 去除中心词\n",
    "        # 每取一个背景词，需要取K倍的负采样\n",
    "        neg_words = torch.multinomial(select_weight, self.K * pos_words.shape[0], True)\n",
    "\n",
    "        return center_word, pos_words, neg_words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = 'text8/text8.train.txt'\n",
    "batch_size = 32\n",
    "lr = 0.2\n",
    "epochs = 2\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_data_loader():\n",
    "    def __init__(self, data_path, batch_size, shuffle=True):\n",
    "        process_data = Process_source_data(data_path)\n",
    "        self.dataset = Skip_gram_Dataset(process_data.text, process_data.word2id, process_data.word_freq)\n",
    "        self.loader = Data.DataLoader(self.dataset, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loader = My_data_loader(data_path, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_size)  #中心词的词向量矩阵\n",
    "        self.out_embed = nn.Embedding(vocab_size, embed_size) #背景词的词向量矩阵\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        input_embedding = self.in_embed(input_labels)  # [bs, embed_size]\n",
    "        input_embedding = input_embedding.unsqueeze(2) # [bs, embed_size, 1]\n",
    "        pos_embedding = self.out_embed(pos_labels)    # [bs, windows * 2 , embed_size]\n",
    "        neg_embedding = self.out_embed(neg_labels)    # [bs, windows * 2 * K, embed_size]\n",
    "        \n",
    "        # 中心词与背景词应该同时出现，因此pos_dot的sigmoid结果应该趋于1\n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding) # [batch_size, (window * 2), 1]\n",
    "        pos_dot = pos_dot.squeeze(2) # [batch_size, (window * 2)]\n",
    "        \n",
    "        # 中心词与噪声词（负采样）不应该同时出现，因此pos_dot的sigmoid结果应该趋于0，\n",
    "        # 但由于sigmoid函数的输出越接近1， logsigmoid的输出越接近0 \n",
    "        # 因此多一个负号， 使得igmoid结果应该趋于1\n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding) # [batch_size, (window * 2 * K), 1]\n",
    "        neg_dot = neg_dot.squeeze(2) # batch_size, (window * 2 * K)]\n",
    "        \n",
    "        # sigmoid函数的输出在为0-1之间， 则logsigmoid的输出全都小于0\n",
    "        # 当sigmoid函数的输出越接近1， 则logsigmoid的输出越接近0\n",
    "        # 可以理解为输出为1的损失小，为0的损失大\n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1) \n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "        \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        # logsigmoid的输出全都小于0， 如果要最小化loss，需要取负号\n",
    "        return -loss\n",
    "    \n",
    "    def input_embedding(self):\n",
    "        # 获取中心词的词向量矩阵\n",
    "        return self.in_embed.weight.detach().numpy()\n",
    "\n",
    "model = Embedding_Model(MAX_VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_embed.weight\n",
      "yes\n",
      "out_embed.weight\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    if 'embed' in name:\n",
    "        print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(1):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(my_loader.loader):\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('epoch', e, 'iteration', i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(word):\n",
    "    index = word2idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx2word[i] for i in cos_dis.argsort()[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in [\"two\", \"america\", \"computer\"]:\n",
    "    print(word, find_nearest(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mul-torch",
   "language": "python",
   "name": "mul-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
