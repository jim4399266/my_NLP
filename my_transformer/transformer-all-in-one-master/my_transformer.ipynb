{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 怎么用 Pytorch 实现一个完整的 Transformer 模型？\n",
    "- Tokenize  \n",
    "- Input Embedding  \n",
    "- Positional Encoder  \n",
    "- Transformer Block  \n",
    "- Encoder  \n",
    "- Decoder  \n",
    "- Transformer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize(object):\n",
    "    \n",
    "    def __init__(self, lang):\n",
    "        self.nlp = importlib.import_module(lang).load()\n",
    "            \n",
    "    def tokenizer(self, sentence):\n",
    "        sentence = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Positional_Encode(nn.Module):\n",
    "    def __init__(self, d_model, sen_len=80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(sen_len, d_model)\n",
    "        \n",
    "        for pos in range(sen_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #让token embeddings大一些，\n",
    "        #这样pos embedding就相对小一些,这样可以减少token embeddings 的方差\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        \n",
    "        seq_len = x.shape(1)\n",
    "        x = x + Variable(self.pe[:, :seq_len], \n",
    "                        requires_grad=False).cuda()\n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=black size=3 face=雅黑>**Transformer Block**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了输入，我们接下来就要开始构建 Transformer Block 了，Transformer Block 主要是有以下4个部分构成的：\n",
    "\n",
    "- self-attention layer\n",
    "- normalization layer\n",
    "- feed forward layer\n",
    "- another normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5316,  0.2449, -0.5929,  0.0301, -0.8545],\n",
       "        [ 0.8579, -0.7765,  0.9545, -0.2580,  0.5755],\n",
       "        [ 1.2618,  0.4861, -1.2943, -0.4913,  0.3012]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  self-attention layer\n",
    "class Multihead_attention(nn.Module):\n",
    "    def __inif__(self, heads, d_model, dropout=0.1):\n",
    "        super().__inif__()\n",
    "        \n",
    "        self.h = heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        \n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        sen_len = q.shape[2]\n",
    "        scores = q.matmul(k.transpose(-2,-1))/math.sqrt(d_k)\n",
    "        # scores: [bs, heads, sen_len, sen_len]\n",
    "        \n",
    "        # mask掉那些为了padding长度增加的token，让其通过softmax计算后为0\n",
    "        if mask is not None:\n",
    "            # mask: [bs, sen_len]\n",
    "            #mask = mask.unsqueeze(1).repeat(1, sen_len, 1).unsqueeze(1)# mask: [bs, 1, sen_len, sen_len]\n",
    "            # 可以直接利用广播机制，这里是self-attention的mask，\n",
    "            # 所以每个时刻都可以attend到所有其它时刻，所有第三维也是1，也使用broadcasting。\n",
    "            # 如果是普通的mask，那么mask的shape是(bs 1, sen_len, sen_len)。\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1) # mask: [bs, 1, 1, sen_len]\n",
    "            \n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "        weight = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            weight = dropout(weight)\n",
    "        output = weight.matmul(v)\n",
    "        return output      \n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.shape[0]\n",
    "        \n",
    "        # transpose to get dimensions bs * N * sl * d_model\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
    "        k = self.q_linear(k).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
    "        v = self.q_linear(v).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        attentioned = self.attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        concat = attentioned.transpose(1,2).contiguous().view(bs, -1, self.model)\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Layer Norm\n",
    "class NormLayer(nn.Module):\n",
    "    '''layer normalization可以将数据分布拉到激活函数的非饱和区，\n",
    "    具有权重/数据伸缩不变性的特点。\n",
    "    起到缓解梯度消失/爆炸、加速训练、正则化的效果。'''\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        # 使用两个可以学习的参数来进行 normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ffn = nn.Sequencial(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.Gelu(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_block(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = NormLayer(d_model)\n",
    "        self.norm2 = NormLayer(d_model)\n",
    "        \n",
    "        self.attention = Multihead_attention(heads, d_model)\n",
    "        self.ffn = FeedForward(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.dropout1(self.attention(x, x, x, mask))\n",
    "        x = self.norm1(z + x)\n",
    "        z = self.dropout2(self.ffn(x))\n",
    "        out = self.norm2(z + x)\n",
    "        return out\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "#         self.tokenizer = Tokenize('cn')\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = Positional_Encode(d_model)\n",
    "        \n",
    "        self.layers = self.get_clones(Encoder_block(d_model, heads), N)\n",
    "        self.norm = NormLayer(d_model)\n",
    "        \n",
    "    def get_clones(module, N):\n",
    "        return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "    \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embedding(src)\n",
    "        x = self.pos_encoder(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_block(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = NormLayer(d_model)\n",
    "        self.norm2 = NormLayer(d_model)\n",
    "        self.norm3 = NormLayer(d_model)\n",
    "        \n",
    "        self.attention1 = Multihead_attention(heads, d_model)\n",
    "        self.attention2 = Multihead_attention(heads, d_model)\n",
    "        self.ffn = FeedForward(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        z = self.dropout1(self.attention1(x, x, x, trg_mask))\n",
    "        x = self.norm1(z + x)\n",
    "        z = self.dropout2(self.attention2(x, e_outputs, e_outputs, src_mask))\n",
    "        x = self.norm2(z + x)\n",
    "        return self.norm3(self.dropout3(x))\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout)\n",
    "     super().__init__()\n",
    "        self.N = N\n",
    "#         self.tokenizer = Tokenize('cn')\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = Positional_Encode(d_model)\n",
    "        \n",
    "        self.layers = self.get_clones(Decoder_block(d_model, heads), N)\n",
    "        self.norm = NormLayer(d_model)\n",
    "        \n",
    "    def get_clones(module, N):\n",
    "        return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "    \n",
    "    def forward(self, trg, e_outpout, src_mask, trg_mask):\n",
    "        x = self.embedding(src)\n",
    "        x = self.pos_encoder(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-14d181d6b282>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_output = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_output, src_mask, trg_mask)\n",
    "        return self.out(d_outpout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequent_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, trg_vocab, d_model=512, N=6, heads=8, dropout=0.1):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mul-torch",
   "language": "python",
   "name": "mul-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
